{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "#Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have\n",
    "#not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between\n",
    "#each transaction and the first transaction in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  isFraud  \n",
       "0 -0.189115  0.133558 -0.021053  149.62        0  \n",
       "1  0.125895 -0.008983  0.014724    2.69        0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66        0  \n",
       "3 -0.221929  0.062723  0.061458  123.50        0  \n",
       "4  0.502292  0.219422  0.215153   69.99        0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns = {'Class':'isFraud'}, inplace = True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.62369e+05, 1.31640e+04, 4.02500e+03, 1.96100e+03, 9.64000e+02,\n",
       "        5.68000e+02, 3.64000e+02, 2.62000e+02, 1.23000e+02, 1.03000e+02,\n",
       "        8.50000e+01, 5.90000e+01, 4.50000e+01, 4.70000e+01, 3.80000e+01,\n",
       "        3.10000e+01, 2.20000e+01, 1.10000e+01, 1.40000e+01, 8.00000e+00,\n",
       "        7.00000e+00, 3.00000e+00, 6.00000e+00, 5.00000e+00, 1.00000e+00,\n",
       "        4.00000e+00, 2.00000e+00, 3.00000e+00, 2.00000e+00, 3.00000e+00,\n",
       "        4.00000e+00, 1.00000e+00, 1.00000e+00, 0.00000e+00, 2.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00]),\n",
       " array([    0.    ,   256.9116,   513.8232,   770.7348,  1027.6464,\n",
       "         1284.558 ,  1541.4696,  1798.3812,  2055.2928,  2312.2044,\n",
       "         2569.116 ,  2826.0276,  3082.9392,  3339.8508,  3596.7624,\n",
       "         3853.674 ,  4110.5856,  4367.4972,  4624.4088,  4881.3204,\n",
       "         5138.232 ,  5395.1436,  5652.0552,  5908.9668,  6165.8784,\n",
       "         6422.79  ,  6679.7016,  6936.6132,  7193.5248,  7450.4364,\n",
       "         7707.348 ,  7964.2596,  8221.1712,  8478.0828,  8734.9944,\n",
       "         8991.906 ,  9248.8176,  9505.7292,  9762.6408, 10019.5524,\n",
       "        10276.464 , 10533.3756, 10790.2872, 11047.1988, 11304.1104,\n",
       "        11561.022 , 11817.9336, 12074.8452, 12331.7568, 12588.6684,\n",
       "        12845.58  , 13102.4916, 13359.4032, 13616.3148, 13873.2264,\n",
       "        14130.138 , 14387.0496, 14643.9612, 14900.8728, 15157.7844,\n",
       "        15414.696 , 15671.6076, 15928.5192, 16185.4308, 16442.3424,\n",
       "        16699.254 , 16956.1656, 17213.0772, 17469.9888, 17726.9004,\n",
       "        17983.812 , 18240.7236, 18497.6352, 18754.5468, 19011.4584,\n",
       "        19268.37  , 19525.2816, 19782.1932, 20039.1048, 20296.0164,\n",
       "        20552.928 , 20809.8396, 21066.7512, 21323.6628, 21580.5744,\n",
       "        21837.486 , 22094.3976, 22351.3092, 22608.2208, 22865.1324,\n",
       "        23122.044 , 23378.9556, 23635.8672, 23892.7788, 24149.6904,\n",
       "        24406.602 , 24663.5136, 24920.4252, 25177.3368, 25434.2484,\n",
       "        25691.16  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df[df.isFraud == False]['Amount'], color='b', label='Real', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([230.,  27.,  14.,   5.,  12.,   8.,   9.,   4.,   5.,   2.,   3.,\n",
       "          6.,   3.,   5.,  38.,   7.,   3.,   5.,   3.,   1.,   3.,   1.,\n",
       "          1.,   0.,   4.,   2.,   5.,   1.,   1.,   2.,   0.,   1.,   1.,\n",
       "          4.,   2.,   3.,   2.,   0.,   3.,   0.,   1.,   2.,   0.,   2.,\n",
       "          4.,   5.,   0.,   1.,   2.,   1.,   3.,   2.,   0.,   0.,   0.,\n",
       "          1.,   0.,   0.,   0.,   0.,   1.,   0.,   1.,   1.,   2.,   0.,\n",
       "          0.,   1.,   0.,   1.,   0.,   0.,   2.,   1.,   1.,   0.,   0.,\n",
       "          1.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,   0.,   0.,\n",
       "          0.,   2.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   3.,   3.,   1.,   0.,   0.,   0.,   0.,   1.,   1.,\n",
       "          0.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   1.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   1.,   1.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   1.]),\n",
       " array([   0.        ,    7.08623333,   14.17246667,   21.2587    ,\n",
       "          28.34493333,   35.43116667,   42.5174    ,   49.60363333,\n",
       "          56.68986667,   63.7761    ,   70.86233333,   77.94856667,\n",
       "          85.0348    ,   92.12103333,   99.20726667,  106.2935    ,\n",
       "         113.37973333,  120.46596667,  127.5522    ,  134.63843333,\n",
       "         141.72466667,  148.8109    ,  155.89713333,  162.98336667,\n",
       "         170.0696    ,  177.15583333,  184.24206667,  191.3283    ,\n",
       "         198.41453333,  205.50076667,  212.587     ,  219.67323333,\n",
       "         226.75946667,  233.8457    ,  240.93193333,  248.01816667,\n",
       "         255.1044    ,  262.19063333,  269.27686667,  276.3631    ,\n",
       "         283.44933333,  290.53556667,  297.6218    ,  304.70803333,\n",
       "         311.79426667,  318.8805    ,  325.96673333,  333.05296667,\n",
       "         340.1392    ,  347.22543333,  354.31166667,  361.3979    ,\n",
       "         368.48413333,  375.57036667,  382.6566    ,  389.74283333,\n",
       "         396.82906667,  403.9153    ,  411.00153333,  418.08776667,\n",
       "         425.174     ,  432.26023333,  439.34646667,  446.4327    ,\n",
       "         453.51893333,  460.60516667,  467.6914    ,  474.77763333,\n",
       "         481.86386667,  488.9501    ,  496.03633333,  503.12256667,\n",
       "         510.2088    ,  517.29503333,  524.38126667,  531.4675    ,\n",
       "         538.55373333,  545.63996667,  552.7262    ,  559.81243333,\n",
       "         566.89866667,  573.9849    ,  581.07113333,  588.15736667,\n",
       "         595.2436    ,  602.32983333,  609.41606667,  616.5023    ,\n",
       "         623.58853333,  630.67476667,  637.761     ,  644.84723333,\n",
       "         651.93346667,  659.0197    ,  666.10593333,  673.19216667,\n",
       "         680.2784    ,  687.36463333,  694.45086667,  701.5371    ,\n",
       "         708.62333333,  715.70956667,  722.7958    ,  729.88203333,\n",
       "         736.96826667,  744.0545    ,  751.14073333,  758.22696667,\n",
       "         765.3132    ,  772.39943333,  779.48566667,  786.5719    ,\n",
       "         793.65813333,  800.74436667,  807.8306    ,  814.91683333,\n",
       "         822.00306667,  829.0893    ,  836.17553333,  843.26176667,\n",
       "         850.348     ,  857.43423333,  864.52046667,  871.6067    ,\n",
       "         878.69293333,  885.77916667,  892.8654    ,  899.95163333,\n",
       "         907.03786667,  914.1241    ,  921.21033333,  928.29656667,\n",
       "         935.3828    ,  942.46903333,  949.55526667,  956.6415    ,\n",
       "         963.72773333,  970.81396667,  977.9002    ,  984.98643333,\n",
       "         992.07266667,  999.1589    , 1006.24513333, 1013.33136667,\n",
       "        1020.4176    , 1027.50383333, 1034.59006667, 1041.6763    ,\n",
       "        1048.76253333, 1055.84876667, 1062.935     , 1070.02123333,\n",
       "        1077.10746667, 1084.1937    , 1091.27993333, 1098.36616667,\n",
       "        1105.4524    , 1112.53863333, 1119.62486667, 1126.7111    ,\n",
       "        1133.79733333, 1140.88356667, 1147.9698    , 1155.05603333,\n",
       "        1162.14226667, 1169.2285    , 1176.31473333, 1183.40096667,\n",
       "        1190.4872    , 1197.57343333, 1204.65966667, 1211.7459    ,\n",
       "        1218.83213333, 1225.91836667, 1233.0046    , 1240.09083333,\n",
       "        1247.17706667, 1254.2633    , 1261.34953333, 1268.43576667,\n",
       "        1275.522     , 1282.60823333, 1289.69446667, 1296.7807    ,\n",
       "        1303.86693333, 1310.95316667, 1318.0394    , 1325.12563333,\n",
       "        1332.21186667, 1339.2981    , 1346.38433333, 1353.47056667,\n",
       "        1360.5568    , 1367.64303333, 1374.72926667, 1381.8155    ,\n",
       "        1388.90173333, 1395.98796667, 1403.0742    , 1410.16043333,\n",
       "        1417.24666667, 1424.3329    , 1431.41913333, 1438.50536667,\n",
       "        1445.5916    , 1452.67783333, 1459.76406667, 1466.8503    ,\n",
       "        1473.93653333, 1481.02276667, 1488.109     , 1495.19523333,\n",
       "        1502.28146667, 1509.3677    , 1516.45393333, 1523.54016667,\n",
       "        1530.6264    , 1537.71263333, 1544.79886667, 1551.8851    ,\n",
       "        1558.97133333, 1566.05756667, 1573.1438    , 1580.23003333,\n",
       "        1587.31626667, 1594.4025    , 1601.48873333, 1608.57496667,\n",
       "        1615.6612    , 1622.74743333, 1629.83366667, 1636.9199    ,\n",
       "        1644.00613333, 1651.09236667, 1658.1786    , 1665.26483333,\n",
       "        1672.35106667, 1679.4373    , 1686.52353333, 1693.60976667,\n",
       "        1700.696     , 1707.78223333, 1714.86846667, 1721.9547    ,\n",
       "        1729.04093333, 1736.12716667, 1743.2134    , 1750.29963333,\n",
       "        1757.38586667, 1764.4721    , 1771.55833333, 1778.64456667,\n",
       "        1785.7308    , 1792.81703333, 1799.90326667, 1806.9895    ,\n",
       "        1814.07573333, 1821.16196667, 1828.2482    , 1835.33443333,\n",
       "        1842.42066667, 1849.5069    , 1856.59313333, 1863.67936667,\n",
       "        1870.7656    , 1877.85183333, 1884.93806667, 1892.0243    ,\n",
       "        1899.11053333, 1906.19676667, 1913.283     , 1920.36923333,\n",
       "        1927.45546667, 1934.5417    , 1941.62793333, 1948.71416667,\n",
       "        1955.8004    , 1962.88663333, 1969.97286667, 1977.0591    ,\n",
       "        1984.14533333, 1991.23156667, 1998.3178    , 2005.40403333,\n",
       "        2012.49026667, 2019.5765    , 2026.66273333, 2033.74896667,\n",
       "        2040.8352    , 2047.92143333, 2055.00766667, 2062.0939    ,\n",
       "        2069.18013333, 2076.26636667, 2083.3526    , 2090.43883333,\n",
       "        2097.52506667, 2104.6113    , 2111.69753333, 2118.78376667,\n",
       "        2125.87      ]),\n",
       " <BarContainer object of 300 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[df.isFraud == True]['Amount'], color='r', label='Fraud', bins=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X, y = df.iloc[:,:-1], df.iloc[:, -1]\n",
    "y = keras.utils.to_categorical(y, num_classes=1)\n",
    "\n",
    "X_orig = X\n",
    "X = norm(X) #normalize the data to N(0,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### build keras neural network models [BE MORE SPECIFIC! ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "mod_focal = Sequential()\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "mod_focal.add(Dense(10, input_dim=input_dim, activation='relu', name='input'))\n",
    "mod_focal.add(Dense(20, activation='relu', name='fc1'))\n",
    "mod_focal.add(Dense(10, activation='relu', name='fc2'))\n",
    "mod_focal.add(Dense(nb_classes, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_ce_acc = tf.keras.models.clone_model(mod_focal)\n",
    "mod_ce_rec = tf.keras.models.clone_model(mod_focal)     \n",
    "mod_ce_bal_weight = tf.keras.models.clone_model(mod_focal)\n",
    "mod_ce_bal= tf.keras.models.clone_model(mod_focal)\n",
    "mod_dectree = tf.keras.models.clone_model(mod_focal)\n",
    "mod_unequal = tf.keras.models.clone_model(mod_focal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focal loss method, source: https://github.com/Tony607/Focal_Loss_Keras\n",
    "\n",
    "class FocalLoss(keras.losses.Loss):\n",
    "    def __init__(self, gamma=2., alpha=4.,\n",
    "                 reduction=keras.losses.Reduction.AUTO, name='focal_loss'):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__(reduction=reduction,\n",
    "                                        name=name)\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(\n",
    "            tf.subtract(1., model_out), self.gamma))\n",
    "        fl = tf.multiply(self.alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asymmetric loss\n",
    "#https://github.com/keras-team/keras/issues/2115\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "class WeightedCategoricalCrossentropy(CategoricalCrossentropy):\n",
    "    \n",
    "    def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
    "        assert cost_mat.ndim == 2\n",
    "        assert cost_mat.shape[0] == cost_mat.shape[1]\n",
    "        \n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cost_mat = K.cast_to_floatx(cost_mat)\n",
    "    \n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        assert sample_weight is None, \"should only be derived from the cost matrix\"\n",
    "      \n",
    "        return super().__call__(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
    "        )\n",
    "\n",
    "\n",
    "def get_sample_weights(y_true, y_pred, cost_m):\n",
    "    num_classes = len(cost_m)\n",
    "\n",
    "    y_pred.shape.assert_has_rank(2)\n",
    "    y_pred.shape[1:].assert_is_compatible_with(num_classes)\n",
    "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
    "\n",
    "    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
    "\n",
    "    y_true_nk1 = K.expand_dims(y_true, 2)\n",
    "    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
    "    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
    "\n",
    "    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
    "    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
    "\n",
    "    return sample_weights_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asymmetric loss\n",
    "#https://github.com/keras-team/keras/issues/2115\n",
    "#\n",
    "#from keras.losses import CategoricalCrossentropy\n",
    "#from tensorflow.python.keras.utils import losses_utils\n",
    "#from itertools import product\n",
    "#\n",
    "## w_array[i, j] defines the weight for an example of class i falsely classified as class j.\n",
    "#def get_sample_weights(y_true, y_pred, cost_m):\n",
    "#    num_classes = len(cost_m)\n",
    "#\n",
    "#    cost_m = K.cast(cost_m, 'float32')\n",
    "#    y_pred.shape.assert_has_rank(2)\n",
    "#    assert(y_pred.shape[1] == num_classes)\n",
    "#    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
    "#\n",
    "#    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
    "#\n",
    "#    y_true_nk1 = K.expand_dims(y_true, 2)\n",
    "#    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
    "#    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
    "#\n",
    "#    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
    "#    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
    "#\n",
    "#    return sample_weights_n\n",
    "#\n",
    "#\n",
    "#class weighted_categorical_crossentropy(tf.keras.losses.CategoricalCrossentropy):\n",
    "#    \n",
    "#  def __init__(\n",
    "#      self,\n",
    "#      *,\n",
    "#      weights,\n",
    "#      from_logits=False,\n",
    "#      label_smoothing=0,\n",
    "#      reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "#      name='categorical_crossentropy',\n",
    "#  ):\n",
    "#\n",
    "#      super().__init__(\n",
    "#          from_logits, label_smoothing, reduction, name=f\"weighted_{name}\"\n",
    "#      )\n",
    "#      self.weights = weights\n",
    "#\n",
    "#  def call(self, y_true, y_pred):\n",
    "#     return super().call(y_true, y_pred) * get_sample_weights(y_true, y_pred, self.weights)\n",
    "#\n",
    "#  def get_config(self):\n",
    "#    return {'weights': self.weights}\n",
    "#\n",
    "#  @classmethod\n",
    "#  def from_config(cls, config):\n",
    "#    return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the models\n",
    "\n",
    "mod_focal.compile(loss=FocalLoss(alpha=1),\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.compile(loss=WeightedCategoricalCrossentropy(cost_matrix), ...) #weighted model.\n",
    "\n",
    "mod_ce_acc.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mod_ce_rec.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight misclassification of fraud higher.\n",
    "w_array = np.ones((2,2))\n",
    "w_array[1,0] = 4\n",
    "\n",
    "\n",
    "mod_ce_bal_weight.compile(loss=WeightedCategoricalCrossentropy(w_array),\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 762\n",
      "Trainable params: 762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "228/228 [==============================] - 1s 2ms/step - loss: 0.0431 - accuracy: 0.9811\n",
      "Epoch 2/3\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0150 - accuracy: 0.9982\n",
      "Epoch 3/3\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0117 - accuracy: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140dc9850>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_focal.summary()\n",
    "mod_focal.fit(X_train, y_train, epochs=3, batch_size=1000) #fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our universal fitting params\n",
    "\n",
    "nb_batch = 1000\n",
    "nb_epoch = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9922\n",
      "Epoch 2/3\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0388 - accuracy: 0.9982\n",
      "Epoch 3/3\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.0267 - accuracy: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13c702d90>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_ce_acc.fit(X_train, y_train, epochs=3, batch_size=nb_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.2426 - accuracy: 0.9975\n",
      "Epoch 2/3\n",
      "228/228 [==============================] - 1s 4ms/step - loss: 0.0436 - accuracy: 0.9982\n",
      "Epoch 3/3\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10572e6a0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_ce_rec.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.9286\n",
      "Epoch 2/3\n",
      "228/228 [==============================] - 1s 3ms/step - loss: 0.1105 - accuracy: 0.9982\n",
      "Epoch 3/3\n",
      "228/228 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13bf175b0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mod_ce_bal_weight.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch)\n",
    "\n",
    "#mod_ce_bal_weight.fit(X_train, y_train, epochs=3, batch_size=1000,\n",
    "#                      sample_weight = ((X_train['Amount'] - min(X_train['Amount'])/max(X_train['Amount'])) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mod_focal.evaluate(X_test, y_test, batch_size=1000) #evaluate fit using ??? method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: instead of confusion matrix, use the ML things they tend to use: accuracy, and th eother two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "predictions = mod_focal.predict(X_test, batch_size=1000)\n",
    "\n",
    "LABELS = ['Real','Fraud'] \n",
    "\n",
    "max_test = np.argmax(y_test, axis=1)\n",
    "max_predictions = np.argmax(predictions, axis=1)\n",
    "confusion_matrix = metrics.confusion_matrix(max_test, max_predictions)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=20)\n",
    "plt.ylabel('True label', fontsize=20)\n",
    "plt.xlabel('Predicted label', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = confusion_matrix.view()\n",
    "error_count = values.sum() - np.trace(values)\n",
    "error_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new metric: amount of money lost by company due to fraud. (assuming they reimburse all fraud cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of fraud cases categorizes as real -> get the amount of money\n",
    "X_temp = X_orig.iloc[X_test.index]['Amount']\n",
    "total_monetary_loss = sum(X_temp[(max_test != max_predictions) & (max_test == 1)])\n",
    "print(\"total monetary loss: \" + str(total_monetary_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
