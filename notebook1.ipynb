{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "#Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have\n",
    "#not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between\n",
    "#each transaction and the first transaction in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  isFraud  \n",
       "0 -0.189115  0.133558 -0.021053  149.62        0  \n",
       "1  0.125895 -0.008983  0.014724    2.69        0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66        0  \n",
       "3 -0.221929  0.062723  0.061458  123.50        0  \n",
       "4  0.502292  0.219422  0.215153   69.99        0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns = {'Class':'isFraud'}, inplace = True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.62369e+05, 1.31640e+04, 4.02500e+03, 1.96100e+03, 9.64000e+02,\n",
       "        5.68000e+02, 3.64000e+02, 2.62000e+02, 1.23000e+02, 1.03000e+02,\n",
       "        8.50000e+01, 5.90000e+01, 4.50000e+01, 4.70000e+01, 3.80000e+01,\n",
       "        3.10000e+01, 2.20000e+01, 1.10000e+01, 1.40000e+01, 8.00000e+00,\n",
       "        7.00000e+00, 3.00000e+00, 6.00000e+00, 5.00000e+00, 1.00000e+00,\n",
       "        4.00000e+00, 2.00000e+00, 3.00000e+00, 2.00000e+00, 3.00000e+00,\n",
       "        4.00000e+00, 1.00000e+00, 1.00000e+00, 0.00000e+00, 2.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00]),\n",
       " array([    0.    ,   256.9116,   513.8232,   770.7348,  1027.6464,\n",
       "         1284.558 ,  1541.4696,  1798.3812,  2055.2928,  2312.2044,\n",
       "         2569.116 ,  2826.0276,  3082.9392,  3339.8508,  3596.7624,\n",
       "         3853.674 ,  4110.5856,  4367.4972,  4624.4088,  4881.3204,\n",
       "         5138.232 ,  5395.1436,  5652.0552,  5908.9668,  6165.8784,\n",
       "         6422.79  ,  6679.7016,  6936.6132,  7193.5248,  7450.4364,\n",
       "         7707.348 ,  7964.2596,  8221.1712,  8478.0828,  8734.9944,\n",
       "         8991.906 ,  9248.8176,  9505.7292,  9762.6408, 10019.5524,\n",
       "        10276.464 , 10533.3756, 10790.2872, 11047.1988, 11304.1104,\n",
       "        11561.022 , 11817.9336, 12074.8452, 12331.7568, 12588.6684,\n",
       "        12845.58  , 13102.4916, 13359.4032, 13616.3148, 13873.2264,\n",
       "        14130.138 , 14387.0496, 14643.9612, 14900.8728, 15157.7844,\n",
       "        15414.696 , 15671.6076, 15928.5192, 16185.4308, 16442.3424,\n",
       "        16699.254 , 16956.1656, 17213.0772, 17469.9888, 17726.9004,\n",
       "        17983.812 , 18240.7236, 18497.6352, 18754.5468, 19011.4584,\n",
       "        19268.37  , 19525.2816, 19782.1932, 20039.1048, 20296.0164,\n",
       "        20552.928 , 20809.8396, 21066.7512, 21323.6628, 21580.5744,\n",
       "        21837.486 , 22094.3976, 22351.3092, 22608.2208, 22865.1324,\n",
       "        23122.044 , 23378.9556, 23635.8672, 23892.7788, 24149.6904,\n",
       "        24406.602 , 24663.5136, 24920.4252, 25177.3368, 25434.2484,\n",
       "        25691.16  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df[df.isFraud == False]['Amount'], color='b', label='Real', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([230.,  27.,  14.,   5.,  12.,   8.,   9.,   4.,   5.,   2.,   3.,\n",
       "          6.,   3.,   5.,  38.,   7.,   3.,   5.,   3.,   1.,   3.,   1.,\n",
       "          1.,   0.,   4.,   2.,   5.,   1.,   1.,   2.,   0.,   1.,   1.,\n",
       "          4.,   2.,   3.,   2.,   0.,   3.,   0.,   1.,   2.,   0.,   2.,\n",
       "          4.,   5.,   0.,   1.,   2.,   1.,   3.,   2.,   0.,   0.,   0.,\n",
       "          1.,   0.,   0.,   0.,   0.,   1.,   0.,   1.,   1.,   2.,   0.,\n",
       "          0.,   1.,   0.,   1.,   0.,   0.,   2.,   1.,   1.,   0.,   0.,\n",
       "          1.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,   0.,   0.,\n",
       "          0.,   2.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   3.,   3.,   1.,   0.,   0.,   0.,   0.,   1.,   1.,\n",
       "          0.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   1.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   1.,   1.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   1.]),\n",
       " array([   0.        ,    7.08623333,   14.17246667,   21.2587    ,\n",
       "          28.34493333,   35.43116667,   42.5174    ,   49.60363333,\n",
       "          56.68986667,   63.7761    ,   70.86233333,   77.94856667,\n",
       "          85.0348    ,   92.12103333,   99.20726667,  106.2935    ,\n",
       "         113.37973333,  120.46596667,  127.5522    ,  134.63843333,\n",
       "         141.72466667,  148.8109    ,  155.89713333,  162.98336667,\n",
       "         170.0696    ,  177.15583333,  184.24206667,  191.3283    ,\n",
       "         198.41453333,  205.50076667,  212.587     ,  219.67323333,\n",
       "         226.75946667,  233.8457    ,  240.93193333,  248.01816667,\n",
       "         255.1044    ,  262.19063333,  269.27686667,  276.3631    ,\n",
       "         283.44933333,  290.53556667,  297.6218    ,  304.70803333,\n",
       "         311.79426667,  318.8805    ,  325.96673333,  333.05296667,\n",
       "         340.1392    ,  347.22543333,  354.31166667,  361.3979    ,\n",
       "         368.48413333,  375.57036667,  382.6566    ,  389.74283333,\n",
       "         396.82906667,  403.9153    ,  411.00153333,  418.08776667,\n",
       "         425.174     ,  432.26023333,  439.34646667,  446.4327    ,\n",
       "         453.51893333,  460.60516667,  467.6914    ,  474.77763333,\n",
       "         481.86386667,  488.9501    ,  496.03633333,  503.12256667,\n",
       "         510.2088    ,  517.29503333,  524.38126667,  531.4675    ,\n",
       "         538.55373333,  545.63996667,  552.7262    ,  559.81243333,\n",
       "         566.89866667,  573.9849    ,  581.07113333,  588.15736667,\n",
       "         595.2436    ,  602.32983333,  609.41606667,  616.5023    ,\n",
       "         623.58853333,  630.67476667,  637.761     ,  644.84723333,\n",
       "         651.93346667,  659.0197    ,  666.10593333,  673.19216667,\n",
       "         680.2784    ,  687.36463333,  694.45086667,  701.5371    ,\n",
       "         708.62333333,  715.70956667,  722.7958    ,  729.88203333,\n",
       "         736.96826667,  744.0545    ,  751.14073333,  758.22696667,\n",
       "         765.3132    ,  772.39943333,  779.48566667,  786.5719    ,\n",
       "         793.65813333,  800.74436667,  807.8306    ,  814.91683333,\n",
       "         822.00306667,  829.0893    ,  836.17553333,  843.26176667,\n",
       "         850.348     ,  857.43423333,  864.52046667,  871.6067    ,\n",
       "         878.69293333,  885.77916667,  892.8654    ,  899.95163333,\n",
       "         907.03786667,  914.1241    ,  921.21033333,  928.29656667,\n",
       "         935.3828    ,  942.46903333,  949.55526667,  956.6415    ,\n",
       "         963.72773333,  970.81396667,  977.9002    ,  984.98643333,\n",
       "         992.07266667,  999.1589    , 1006.24513333, 1013.33136667,\n",
       "        1020.4176    , 1027.50383333, 1034.59006667, 1041.6763    ,\n",
       "        1048.76253333, 1055.84876667, 1062.935     , 1070.02123333,\n",
       "        1077.10746667, 1084.1937    , 1091.27993333, 1098.36616667,\n",
       "        1105.4524    , 1112.53863333, 1119.62486667, 1126.7111    ,\n",
       "        1133.79733333, 1140.88356667, 1147.9698    , 1155.05603333,\n",
       "        1162.14226667, 1169.2285    , 1176.31473333, 1183.40096667,\n",
       "        1190.4872    , 1197.57343333, 1204.65966667, 1211.7459    ,\n",
       "        1218.83213333, 1225.91836667, 1233.0046    , 1240.09083333,\n",
       "        1247.17706667, 1254.2633    , 1261.34953333, 1268.43576667,\n",
       "        1275.522     , 1282.60823333, 1289.69446667, 1296.7807    ,\n",
       "        1303.86693333, 1310.95316667, 1318.0394    , 1325.12563333,\n",
       "        1332.21186667, 1339.2981    , 1346.38433333, 1353.47056667,\n",
       "        1360.5568    , 1367.64303333, 1374.72926667, 1381.8155    ,\n",
       "        1388.90173333, 1395.98796667, 1403.0742    , 1410.16043333,\n",
       "        1417.24666667, 1424.3329    , 1431.41913333, 1438.50536667,\n",
       "        1445.5916    , 1452.67783333, 1459.76406667, 1466.8503    ,\n",
       "        1473.93653333, 1481.02276667, 1488.109     , 1495.19523333,\n",
       "        1502.28146667, 1509.3677    , 1516.45393333, 1523.54016667,\n",
       "        1530.6264    , 1537.71263333, 1544.79886667, 1551.8851    ,\n",
       "        1558.97133333, 1566.05756667, 1573.1438    , 1580.23003333,\n",
       "        1587.31626667, 1594.4025    , 1601.48873333, 1608.57496667,\n",
       "        1615.6612    , 1622.74743333, 1629.83366667, 1636.9199    ,\n",
       "        1644.00613333, 1651.09236667, 1658.1786    , 1665.26483333,\n",
       "        1672.35106667, 1679.4373    , 1686.52353333, 1693.60976667,\n",
       "        1700.696     , 1707.78223333, 1714.86846667, 1721.9547    ,\n",
       "        1729.04093333, 1736.12716667, 1743.2134    , 1750.29963333,\n",
       "        1757.38586667, 1764.4721    , 1771.55833333, 1778.64456667,\n",
       "        1785.7308    , 1792.81703333, 1799.90326667, 1806.9895    ,\n",
       "        1814.07573333, 1821.16196667, 1828.2482    , 1835.33443333,\n",
       "        1842.42066667, 1849.5069    , 1856.59313333, 1863.67936667,\n",
       "        1870.7656    , 1877.85183333, 1884.93806667, 1892.0243    ,\n",
       "        1899.11053333, 1906.19676667, 1913.283     , 1920.36923333,\n",
       "        1927.45546667, 1934.5417    , 1941.62793333, 1948.71416667,\n",
       "        1955.8004    , 1962.88663333, 1969.97286667, 1977.0591    ,\n",
       "        1984.14533333, 1991.23156667, 1998.3178    , 2005.40403333,\n",
       "        2012.49026667, 2019.5765    , 2026.66273333, 2033.74896667,\n",
       "        2040.8352    , 2047.92143333, 2055.00766667, 2062.0939    ,\n",
       "        2069.18013333, 2076.26636667, 2083.3526    , 2090.43883333,\n",
       "        2097.52506667, 2104.6113    , 2111.69753333, 2118.78376667,\n",
       "        2125.87      ]),\n",
       " <BarContainer object of 300 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[df.isFraud == True]['Amount'], color='r', label='Fraud', bins=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clean data and generate train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X, y = df.iloc[:,:-1], df.iloc[:, -1]\n",
    "y_numeric = y\n",
    "y = keras.utils.to_categorical(y, num_classes=2) #one-hot encoded for use with categorial cross entropy loss\n",
    "\n",
    "X_orig = X\n",
    "X = norm(X) #normalize the data to N(0,1)\n",
    "###UNFINISHED MAYBE WE SHOULDNT NORMALIZXE THE PCA FEATURE COLS\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "#corresponding for resampling methods (we need to use different datatype)\n",
    "y_num_train = y_numeric[X_train.index]\n",
    "y_num_test = y_numeric[X_test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling Methods : SMOTE (oversampling) & nearmiss (undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "SMOTE Algorithm has oversampled the minority instances and made it equal to majority class. Both categories have equal amount of records. More specifically, the minority class has been increased to the total number of majority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss \n",
    "nr = NearMiss() \n",
    "\n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_num_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating our Loss functions: Focal Loss & Asymmetric Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# focal loss method, source: https://github.com/Tony607/Focal_Loss_Keras\n",
    "\n",
    "class FocalLoss(keras.losses.Loss):\n",
    "    def __init__(self, gamma=2., alpha=4.,\n",
    "                 reduction=keras.losses.Reduction.AUTO, name='focal_loss'):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__(reduction=reduction,\n",
    "                                        name=name)\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(\n",
    "            tf.subtract(1., model_out), self.gamma))\n",
    "        fl = tf.multiply(self.alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asymmetric loss\n",
    "#https://github.com/keras-team/keras/issues/2115\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "class WeightedCategoricalCrossentropy(CategoricalCrossentropy):\n",
    "    \n",
    "    def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
    "        assert cost_mat.ndim == 2\n",
    "        assert cost_mat.shape[0] == cost_mat.shape[1]\n",
    "        \n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cost_mat = K.cast_to_floatx(cost_mat)\n",
    "    \n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        assert sample_weight is None, \"should only be derived from the cost matrix\"\n",
    "      \n",
    "        return super().__call__(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
    "        )\n",
    "    \n",
    "#class CashCrossentropy(CategoricalCrossentropy):\n",
    "#    \n",
    "#    def __init__(self, cost_mat, name='weighted_categorical_crossentropy', **kwargs):\n",
    "#        assert cost_mat.ndim == 2\n",
    "#        assert cost_mat.shape[0] == cost_mat.shape[1]\n",
    "#        \n",
    "#        super().__init__(name=name, **kwargs)\n",
    "#        self.cost_mat = K.cast_to_floatx(cost_mat)\n",
    "#    \n",
    "#    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "#        # assert sample_weight is None, \"should only be derived from the cost matrix\"\n",
    "#        \n",
    "#        return super().__call__(\n",
    "#            y_true=y_true,\n",
    "#            y_pred=y_pred,\n",
    "#            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat) * K.cast_to_floatx(sample_weight),\n",
    "#        )\n",
    "\n",
    "\n",
    "def get_sample_weights(y_true, y_pred, cost_m):\n",
    "    num_classes = len(cost_m)\n",
    "\n",
    "    y_pred.shape.assert_has_rank(2)\n",
    "    y_pred.shape[1:].assert_is_compatible_with(num_classes)\n",
    "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
    "\n",
    "    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
    "\n",
    "    y_true_nk1 = K.expand_dims(y_true, 2)\n",
    "    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
    "    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
    "\n",
    "    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
    "    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
    "\n",
    "    return sample_weights_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### build our neural network model, create different instances for each method to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "mod_focal = Sequential()\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "#general CNN model.\n",
    "###DECIDE WHAT KIND CNN TO BUILD###UNFINISHED\n",
    "mod_focal.add(Dense(10, input_dim=input_dim, activation='relu', name='input'))\n",
    "mod_focal.add(Dense(20, activation='relu', name='fc1'))\n",
    "mod_focal.add(Dense(10, activation='relu', name='fc2'))\n",
    "mod_focal.add(Dense(nb_classes, activation='softmax', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss models\n",
    "mod_ce_acc = tf.keras.models.clone_model(mod_focal)\n",
    "mod_ce_bal_weight = tf.keras.models.clone_model(mod_focal) #UNFINISHED probably jsut delete it doesnt work.\n",
    "mod_ce_bal= tf.keras.models.clone_model(mod_focal) #asymmetric cost\n",
    "mod_ce_weight = tf.keras.models.clone_model(mod_focal) #weighted based on cost\n",
    "\n",
    "#resampling models\n",
    "mod_SMOTE = tf.keras.models.clone_model(mod_focal)\n",
    "mod_nearmiss = tf.keras.models.clone_model(mod_focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define common terms for compiling models\n",
    "ourmetrics=['accuracy','AUC','Recall',tf.keras.metrics.FalsePositives(),tf.keras.metrics.FalseNegatives(), tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives()]\n",
    "opt= \"sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the models\n",
    "\n",
    "mod_focal.compile(loss=FocalLoss(alpha=1),\n",
    "              optimizer=opt,\n",
    "              metrics=ourmetrics)\n",
    "\n",
    "mod_ce_acc.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=ourmetrics)\n",
    "\n",
    "mod_ce_weight.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=ourmetrics)\n",
    "\n",
    "mod_SMOTE.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=ourmetrics)\n",
    "\n",
    "mod_nearmiss.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=ourmetrics)\n",
    "\n",
    "\n",
    "#define matrix for Aysmmetric\n",
    "w_array = np.ones((2,2))\n",
    "w_array[1,0] = 4\n",
    "\n",
    "\n",
    "mod_ce_bal.compile(loss=WeightedCategoricalCrossentropy(w_array),\n",
    "              optimizer=opt,\n",
    "              metrics=ourmetrics)\n",
    "\n",
    "#mod_ce_bal_weight.compile(loss=CashCrossentropy(w_array),\n",
    "#              optimizer=opt,\n",
    "#              metrics=ourmetrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up callbacks\n",
    "\n",
    "import time\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our universal fitting params\n",
    "nb_batch = 1000\n",
    "nb_epoch = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0020 - accuracy: 0.9989 - auc: 0.9996 - recall: 0.9989 - false_positives_1: 483.0000 - false_negatives: 483.0000 - true_positives: 455207.0000 - true_negatives: 455207.0000\n",
      "Epoch 2/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9989 - auc: 0.9996 - recall: 0.9989 - false_positives_1: 242.0000 - false_negatives: 242.0000 - true_positives: 227603.0000 - true_negatives: 227603.0000\n",
      "Epoch 3/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0020 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 238.0000 - false_negatives: 238.0000 - true_positives: 227607.0000 - true_negatives: 227607.0000\n",
      "Epoch 4/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0019 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 237.0000 - false_negatives: 237.0000 - true_positives: 227608.0000 - true_negatives: 227608.0000\n",
      "Epoch 5/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 236.0000 - false_negatives: 236.0000 - true_positives: 227609.0000 - true_negatives: 227609.0000\n",
      "Epoch 6/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 232.0000 - false_negatives: 232.0000 - true_positives: 227613.0000 - true_negatives: 227613.0000\n",
      "Epoch 7/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 231.0000 - false_negatives: 231.0000 - true_positives: 227614.0000 - true_negatives: 227614.0000\n",
      "Epoch 8/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0019 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 228.0000 - false_negatives: 228.0000 - true_positives: 227617.0000 - true_negatives: 227617.0000\n"
     ]
    }
   ],
   "source": [
    "time_focal = TimeHistory()\n",
    "hist_focal = mod_focal.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch, callbacks=[time_focal]) #fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0116 - accuracy: 0.9982 - auc: 0.9994 - recall: 0.9982 - false_positives_1: 633.0000 - false_negatives: 633.0000 - true_positives: 455057.0000 - true_negatives: 455057.0000\n",
      "Epoch 2/8\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0110 - accuracy: 0.9982 - auc: 0.9995 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 3/8\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0105 - accuracy: 0.9982 - auc: 0.9995 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 4/8\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0100 - accuracy: 0.9982 - auc: 0.9995 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 5/8\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0095 - accuracy: 0.9982 - auc: 0.9995 - recall: 0.9982 - false_positives_1: 403.0000 - false_negatives: 403.0000 - true_positives: 227442.0000 - true_negatives: 227442.0000\n",
      "Epoch 6/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0090 - accuracy: 0.9983 - auc: 0.9996 - recall: 0.9983 - false_positives_1: 394.0000 - false_negatives: 394.0000 - true_positives: 227451.0000 - true_negatives: 227451.0000\n",
      "Epoch 7/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0086 - accuracy: 0.9983 - auc: 0.9996 - recall: 0.9983 - false_positives_1: 391.0000 - false_negatives: 391.0000 - true_positives: 227454.0000 - true_negatives: 227454.0000\n",
      "Epoch 8/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0083 - accuracy: 0.9983 - auc: 0.9996 - recall: 0.9983 - false_positives_1: 384.0000 - false_negatives: 384.0000 - true_positives: 227461.0000 - true_negatives: 227461.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1541a6460>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_ce_acc.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch, callbacks=[time_focal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0167 - accuracy: 0.9982 - auc: 0.9986 - recall: 0.9982 - false_positives_1: 789.0000 - false_negatives: 789.0000 - true_positives: 454901.0000 - true_negatives: 454901.0000\n",
      "Epoch 2/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0157 - accuracy: 0.9982 - auc: 0.9988 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 3/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0149 - accuracy: 0.9982 - auc: 0.9989 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 4/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0143 - accuracy: 0.9982 - auc: 0.9990 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 5/8\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0138 - accuracy: 0.9982 - auc: 0.9991 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 6/8\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0133 - accuracy: 0.9982 - auc: 0.9992 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 7/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0129 - accuracy: 0.9982 - auc: 0.9992 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 8/8\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0126 - accuracy: 0.9982 - auc: 0.9993 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15573b790>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_nearmiss.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch, callbacks=[time_focal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0183 - accuracy: 0.9982 - auc: 0.9985 - recall: 0.9982 - false_positives_1: 810.0000 - false_negatives: 810.0000 - true_positives: 454880.0000 - true_negatives: 454880.0000\n",
      "Epoch 2/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0168 - accuracy: 0.9982 - auc: 0.9987 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 3/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0157 - accuracy: 0.9982 - auc: 0.9989 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 4/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0148 - accuracy: 0.9982 - auc: 0.9990 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 5/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0140 - accuracy: 0.9982 - auc: 0.9991 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 6/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0133 - accuracy: 0.9982 - auc: 0.9992 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 7/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0128 - accuracy: 0.9982 - auc: 0.9993 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n",
      "Epoch 8/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0123 - accuracy: 0.9982 - auc: 0.9994 - recall: 0.9982 - false_positives_1: 405.0000 - false_negatives: 405.0000 - true_positives: 227440.0000 - true_negatives: 227440.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x155d2f550>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_SMOTE.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch, callbacks=[time_focal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "228/228 [==============================] - 2s 9ms/step - loss: 0.0189 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 644.0000 - false_negatives: 644.0000 - true_positives: 455046.0000 - true_negatives: 455046.0000\n",
      "Epoch 2/8\n",
      "228/228 [==============================] - 2s 8ms/step - loss: 0.0177 - accuracy: 0.9990 - auc: 0.9996 - recall: 0.9990 - false_positives_1: 221.0000 - false_negatives: 221.0000 - true_positives: 227624.0000 - true_negatives: 227624.0000\n",
      "Epoch 3/8\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0168 - accuracy: 0.9991 - auc: 0.9996 - recall: 0.9991 - false_positives_1: 204.0000 - false_negatives: 204.0000 - true_positives: 227641.0000 - true_negatives: 227641.0000\n",
      "Epoch 4/8\n",
      "228/228 [==============================] - 2s 7ms/step - loss: 0.0160 - accuracy: 0.9991 - auc: 0.9996 - recall: 0.9991 - false_positives_1: 196.0000 - false_negatives: 196.0000 - true_positives: 227649.0000 - true_negatives: 227649.0000\n",
      "Epoch 5/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0154 - accuracy: 0.9992 - auc: 0.9997 - recall: 0.9992 - false_positives_1: 189.0000 - false_negatives: 189.0000 - true_positives: 227656.0000 - true_negatives: 227656.0000\n",
      "Epoch 6/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0149 - accuracy: 0.9992 - auc: 0.9997 - recall: 0.9992 - false_positives_1: 180.0000 - false_negatives: 180.0000 - true_positives: 227665.0000 - true_negatives: 227665.0000\n",
      "Epoch 7/8\n",
      "228/228 [==============================] - 1s 5ms/step - loss: 0.0144 - accuracy: 0.9992 - auc: 0.9997 - recall: 0.9992 - false_positives_1: 177.0000 - false_negatives: 177.0000 - true_positives: 227668.0000 - true_negatives: 227668.0000\n",
      "Epoch 8/8\n",
      "228/228 [==============================] - 1s 6ms/step - loss: 0.0141 - accuracy: 0.9993 - auc: 0.9997 - recall: 0.9993 - false_positives_1: 170.0000 - false_negatives: 170.0000 - true_positives: 227675.0000 - true_negatives: 227675.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x155e4d3a0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_ce_bal.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch, callbacks=[time_focal])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_ce_bal_weight.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch,\n",
    "#                      sample_weight = ((X_train['Amount'] - min(X_train['Amount'])/max(X_train['Amount']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/output/Softmax:0) = ] [[nan nan][nan...]...] [y (Cast_6/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]] [Op:__inference_train_function_167287]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f1f232b29c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m mod_ce_weight.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch, callbacks=[time_focal],\n\u001b[0m\u001b[1;32m      6\u001b[0m                       sample_weight = ((X_train['Amount'] - min(X_train['Amount'])/max(X_train['Amount']))))\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/output/Softmax:0) = ] [[nan nan][nan...]...] [y (Cast_6/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]] [Op:__inference_train_function_167287]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "mod_ce_bal.compile(loss=WeightedCategoricalCrossentropy(w_array),\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mod_ce_weight.fit(X_train, y_train, epochs=nb_epoch, batch_size=nb_batch, callbacks=[time_focal],\n",
    "                      sample_weight = ((X_train['Amount'] - min(X_train['Amount'])/max(X_train['Amount']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mod_focal.evaluate(X_test, y_test, batch_size=1000) #evaluate fit using ??? method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: instead of confusion matrix, use the ML things they tend to use: accuracy, and th eother two.\n",
    "#we are concerned with minimized false negatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "predictions = mod_focal.predict(X_test, batch_size=1000)\n",
    "\n",
    "LABELS = ['Real','Fraud'] \n",
    "\n",
    "max_test = np.argmax(y_test, axis=1)\n",
    "max_predictions = np.argmax(predictions, axis=1)\n",
    "confusion_matrix = metrics.confusion_matrix(max_test, max_predictions)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=20)\n",
    "plt.ylabel('True label', fontsize=20)\n",
    "plt.xlabel('Predicted label', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = confusion_matrix.view()\n",
    "error_count = values.sum() - np.trace(values)\n",
    "error_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new metric: amount of money lost by company due to fraud. (assuming they reimburse all fraud cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of fraud cases categorizes as real -> get the amount of money\n",
    "X_temp = X_orig.iloc[X_test.index]['Amount']\n",
    "total_monetary_loss = sum(X_temp[(max_test != max_predictions) & (max_test == 1)])\n",
    "print(\"total monetary loss: \" + str(total_monetary_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_focal.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots , https://wandb.ai/site/articles/plotting-keras-history\n",
    "#https://stackoverflow.com/questions/41908379/keras-plot-training-validation-and-test-set-accuracy\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = hist_focal.history['loss']\n",
    "test_loss = hist_focal.history['accuracy']\n",
    "\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.cumsum(time_focal.times)\n",
    "plt.plot(test_loss,temp, 'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_count,temp, 'r--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
